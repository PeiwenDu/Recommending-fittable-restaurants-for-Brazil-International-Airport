# -*- coding: utf-8 -*-
"""feature_extraction+models+suggestion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F2gaRY8Yu-ldbOUdIgJhnunrd0zuwqFD

# System Setting
"""

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

import os
from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/Shared drives/INF560/new_feature_extraction_data"

os.chdir(path)
os.listdir(path)

!pip install nltk

import numpy as np
import pandas as pd 
import nltk
import re

"""# Data Preprocess

**Review Data Pre-Process**
"""

filename = "reviews.csv"
outputfile = "clean_reviews.csv"

raw = pd.read_csv(filename)

raw.head()

"""Clean and split sentences into words"""

def clean_text(df):
    text = str(df["reviews"])
    text = text.lower()
    text = re.sub("[^a-z\s]", "", text)
    return text

cleaned = raw
cleaned["reviews"] = cleaned.apply(clean_text, axis=1)

cleaned.head()

"""Eliminate stop words and do the stemming"""

nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
stopWords = set(stopwords.words('english'))
stemmer = PorterStemmer()

def reduce_stop_words(df):
  text = str(df["reviews"])
  new_text = " "
  for w in text.split():
    if w not in stopWords:
      new_text = new_text+" "+stemmer.stem(w)
  return new_text

cleaned["reviews"] = cleaned.apply(reduce_stop_words,axis = 1)
cleaned.head

cleaned["review_size"] = cleaned["reviews"].str.count(" ")
cleaned["review_size"].describe()

cleaned["review_size"].plot(title="Number of words per review", figsize=(20, 8))

del cleaned["review_size"]

"""Store the result"""

cleaned.to_csv(outputfile,index=False)

"""# Feature extraction(TF-IDF)

## Calculate TF-IDF matrix
"""

!pip install -U scikit-learn

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer

reviews = pd.read_csv("clean_reviews.csv")
reviews.head

# parameters
# min number of times of term t has to occur in all sentences
min_df = 2
# Number of sentences in a training or testing corpus
n_d = len(reviews)
# maximum number of times of term t can occur in all the sentences which is calculated as max_df*n_d
max_df = 2 * n_d

vectorizer = CountVectorizer(stop_words = 'english',min_df=min_df,max_df = max_df)
X = vectorizer.fit_transform(reviews["reviews"])
word = vectorizer.get_feature_names()
totoal_word = len(word)
totoal_word
X.toarray()

transformer = TfidfTransformer()
tfidf = transformer.fit_transform(X)
tf_idf_values = tfidf.toarray()
tf_idf_values

values = pd.DataFrame(tf_idf_values)
values.to_csv("tf_idf_values.csv",index=False)

"""## Sort tf-idf value

Set TF-IDF value for each review's word and sort the words by their values
"""

word_features = []
for i in range(len(tf_idf_values)):
    fe = []
    for j in range(len(word)):
        fe.append((word[j],tf_idf_values[i][j]))
    word_features.append(sorted(fe,key=lambda x:x[1],reverse = True))

feature_for_each_restaurants = pd.DataFrame()
keywords = []
for i in range(len(word_features)):
    k = ""
    for w in word_features[i]:
        if w[1]>0:
          k=k+" "+w[0]
            # k.append(w[0])
    keywords.append(k)
feature_for_each_restaurants["restaurant"] = reviews["restaurants"]
feature_for_each_restaurants["keyword"] = keywords
feature_for_each_restaurants.to_csv("features.csv",index=False)
feature_for_each_restaurants.head

total_word_length = feature_for_each_restaurants["keyword"].str.count(" ")
total_word_length.describe()
total_word_length.plot(title="Number of features per review", figsize=(20, 8))

"""## Select features"""

amount = 148
retain_words = []
for w in feature_for_each_restaurants["keyword"]:
  retain_words.append(" ".join(w.split(" ")[:149]))
feature_for_each_restaurants["keyword"] = retain_words
feature_for_each_restaurants.to_csv("top_words_feature.csv",index=False)

feature_for_each_restaurants.head

"""# Feature numberized(word2vec)

## Train word2vec model
"""

import pandas as pd
# load the data into panda dataframe
data_file_name = "yelp_review.json"
chunks = pd.read_json(data_file_name,lines=True, chunksize=100000)
print("Data loaded")

# Convert all the review text into a long string and print its length
raw_corpus = ""
for chunk in chunks:
    raw_corpus = u"".join(chunk['text']+" ")
print("Raw Corpus contains {0:,} characters".format(len(raw_corpus)))

# import natural language toolkit
import nltk

# download the punkt tokenizer
nltk.download('punkt')
print("The punkt tokenizer is downloaded")

# Load the punkt tokenizer
tokenizer = nltk.data.load("tokenizers/punkt/english.pickle")
print("The punkt tokenizer is loaded")

# we tokenize the raw string into raw sentences
raw_sentences = tokenizer.tokenize(raw_corpus)
print("We have {0:,} raw sentences".format(len(raw_sentences)))

import re

# Clean and split sentence into words
def clean_and_split_str(string):
    strip_special_chars = re.compile("[^A-Za-z]+")
    string = re.sub(strip_special_chars, " ", string)
    return string.strip().split()

# clean each raw sentences and build the list of sentences
sentences = []
for raw_sent in raw_sentences:
    if len(raw_sent) > 0:
        sentences.append(clean_and_split_str(raw_sent))

reviewsDF = pd.read_csv("clean_reviews.csv")
sentences.extend(reviewsDF["reviews"])
print("We have {0:,} clean sentences".format(len(sentences)))

token_count = sum([len(sentence) for sentence in sentences])
print("The dataset corpus contains {0:,} tokens".format(token_count))

import multiprocessing

# Dimensionality of the resulting word vectors
num_features = 225

# Minimum word count threshold
min_word_count = 3

# Number of threads to run in parallel
num_workers = multiprocessing.cpu_count()

# Context window length
context_size = 7

# Seed for the RNG, to make the result reproducible
seed = 1

import gensim

word2vec_model = gensim.models.word2vec.Word2Vec(
    sg=1,
    seed=seed,
    workers=num_workers,
    size=num_features,
    min_count=min_word_count,
    window=context_size)

word2vec_model.build_vocab(sentences=sentences)
print("The vocabulary is built")
print("Word2Vec vocabulary length: ", word2vec_model.corpus_total_words)

# Start training the model
word2vec_model.train(sentences=sentences,total_examples=word2vec_model.corpus_count,epochs=word2vec_model.iter)
print("Training finished")

#Save the model
word2vec_model.save("word2vec_model_trained_on_yelp_review"+str(num_features)+".w2v")
print("Model saved")

"""## Feature numberized"""

# Load our word2vec model
import gensim
num_features =300
c = 200
w2v_model = gensim.models.word2vec.Word2Vec.load("word2vec_model_trained_on_yelp_review"+str(num_features)+".w2v")
print("Model loaded")

"""Method 1: Transfer features into vectors and each review document is an vector list, used for CNN"""

reviews = pd.read_csv("top_words_feature.csv")

w2v_for_each_restaurant = []
for ws in reviews["keyword"]:
    w2vs = []
    count = 0
    for w in ws.split(" "):
      if count >=c:
        break
      w2v = np.zeros(num_features,dtype=float)
      if w in w2v_model:
        w2v = w2v_model[w]
      count+=1
      w2vs.append(w2v)     
    w2v_for_each_restaurant.append(w2vs)
len(w2v_for_each_restaurant[0])

"""Method 2: Transfer features into vectors and sum up those word vectors, each review is presented by an vector, used for SVM and Logistic regression"""

reviews = pd.read_csv("top_words_feature.csv")

w2v_for_each_restaurant = []
for ws in reviews["keyword"]:
    w2vs = np.zeros(num_features,dtype=float)
    count = 0
    for w in ws.split(" "):
      if count >=c:
        break
      if w in w2v_model:
        w2vs+= w2v_model[w]
      count+=1
    w2v_for_each_restaurant.append(w2vs)
w2v_for_each_restaurant[0]

"""# Rating defined

## KNN

To rate those restaurant which can't get customer traffic from Kiana data
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
# from sklearn.decomposition import PCA
import numpy as np

exist_rating = [1,1,1,3,5,1,1,1,5,5,3,5,5,1,5,5,5,3,3,3,5,5]
exist_index = [1,3,5,6,8,10,11,12,13,16,18,20,23,24,25,26,27,28,29,32,33,34]
# exist_rating = [1,1,3,5,1,1,1,5,5,3,5,1,5,5,5,5,3,3,3,5,5,1]
# exist_index = [2,4,5,7,9,10,11,12,15,17,19,21,22,24,25,26,27,28,31,32,33,35]
# exist_rating = [3,1,3,3,1,5,3,5,5,5,1]
# exist_index = [1,6,8,20,23,25,28,29,32,33,34]
exist_reviews = []
left_reviews = []
left_rating = []
values = pd.read_csv("tf_idf_values.csv")
for l in range(36):
    if l in exist_index:
      exist_reviews.append(values.iloc[l])
    else:
      left_reviews.append(values.iloc[l])
# pca = PCA(n_components=2)
# reduced_dimensional_values = pca.fit_transform(values)
# reduced_dimensional_values
# reviews = pd.read_csv("features.csv")
# whole = []
# for ws in reviews["keyword"]:
#     w2vs = np.zeros(num_features,dtype=float)
#     count = 0
#     for w in ws.split(" "):
#       count+=1
#       if w in w2v_model:
#         w2vs+= w2v_model[w]
#     whole.append(w2vs)

# reduced_dimensional_values = whole

# for l in range(36):
#     if l in exist_index:
#       exist_reviews.append(reduced_dimensional_values[l])
#     else:
#       left_reviews.append(reduced_dimensional_values[l])

exist_reviews = np.array(exist_reviews)
left_reviews = np.array(left_reviews)

neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(exist_reviews, exist_rating)
neigh

# neigh = RandomForestClassifier(max_depth=2, random_state=0)
# neigh.fit(exist_reviews, exist_rating)

left_rating = neigh.predict(left_reviews)
ratings = []
j=0
k=0
for i in range(36):
    if i in exist_index:
        ratings.append(exist_rating[j])
        j+=1
    else:
        ratings.append(left_rating[k])
        k+=1

ratings = pd.DataFrame(ratings)
ratings
ratings.to_csv("lratings_w2v.csv",index=False)

"""## t-SNE"""

import matplotlib.pyplot as plt
from matplotlib import offsetbox

"""Classify restaurants according to TF-IDF values"""

from sklearn.manifold import TSNE
# X = w2v_for_each_restaurant
X = pd.read_csv("tf_idf_values.csv")
rating = pd.read_csv("lratings_w2v.csv")
co = rating['0']
# X = whole
X_tsne = TSNE(n_components=2,perplexity=5,n_iter=100000,early_exaggeration=70,learning_rate=10,method="exact").fit_transform(X)
x_min, x_max = X_tsne.min(0), X_tsne.max(0)
X_norm = (X_tsne - x_min) / (x_max - x_min)
y = reviews["restaurant"]
plt.figure(figsize=(12, 12))
for i in range(X_norm.shape[0]):
  plt.text(X_norm[i, 0], X_norm[i, 1], y[i], color=plt.cm.Set1(co[i]+1), 
             fontdict={'weight': 'bold', 'size': 12})
plt.xticks([])
plt.yticks([])
plt.show()

"""Classify restaurants based on the whole reviews"""

# X = w2v_for_each_restaurant
X = whole
X_tsne = TSNE(n_components=2,perplexity=5,n_iter=100000,early_exaggeration=60,learning_rate=10,method="exact",init="pca").fit_transform(X)
x_min, x_max = X_tsne.min(0), X_tsne.max(0)
X_norm = (X_tsne - x_min) / (x_max - x_min)
y = reviews["restaurant"]
rating = pd.read_csv("lratings_w2v.csv")
co = rating['0']
plt.figure(figsize=(12, 12))
for i in range(X_norm.shape[0]):
  plt.text(X_norm[i, 0], X_norm[i, 1], y[i], color=plt.cm.Set1(co[i]+1), 
             fontdict={'weight': 'bold', 'size': 12})
plt.xticks([])
plt.yticks([])
plt.show()

"""Classify restaurants based on the features we extracted"""

X = w2v_for_each_restaurant
# X = whole
X_tsne = TSNE(n_components=2,perplexity=5,n_iter=100000,early_exaggeration=70,learning_rate=10,method="exact").fit_transform(X)
x_min, x_max = X_tsne.min(0), X_tsne.max(0)
X_norm = (X_tsne - x_min) / (x_max - x_min)
y = reviews["restaurant"]
rating = pd.read_csv("lratings_w2v.csv")
co = rating['0']
plt.figure(figsize=(12, 12))
for i in range(X_norm.shape[0]):
  plt.text(X_norm[i, 0], X_norm[i, 1], y[i], color=plt.cm.Set1(co[i]+1), 
             fontdict={'weight': 'bold', 'size': 12})
plt.xticks([])
plt.yticks([])
plt.show()

"""# Models"""

from joblib import dump

rating = pd.read_csv("lratings_w2v.csv")
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
X=w2v_for_each_restaurant
y=rating
X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=3,shuffle=True)

X_train

"""## SVM"""

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
#adjust parameters
model = SVC(kernel='rbf', probability=True)    
param_grid = {'kernel':['rbf','linear','poly'],'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001,0.1,0.5]}    
grid_search = GridSearchCV(model, param_grid, n_jobs = 8, verbose=1)    
grid_search.fit(X_train, y_train)    
best_parameters = grid_search.best_estimator_.get_params()    
for para, val in list(best_parameters.items()):    
    print(para, val)    
svm = SVC(kernel=best_parameters['kernel'], C=best_parameters['C'], gamma=best_parameters['gamma'],probability=True)

#train SVM model
svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)
dump(svm, 'svm_try.joblib')
# clf = load('filename.joblib')

print("Test accuracy",accuracy_score(y_test, y_pred))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

labels = ['Bad','Medium','Good']

y_true = y_test
y_pred = y_pred

tick_marks = np.array(range(len(labels))) + 0.5

def plot_confusion_matrix(cm, title='Confusion Matrix', cmap=plt.cm.binary):
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(title)
    plt.colorbar()
    xlocations = np.array(range(len(labels)))
    plt.xticks(xlocations, labels)
    plt.yticks(xlocations, labels)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

cm = confusion_matrix(y_true, y_pred)
np.set_printoptions(precision=2)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
print (cm_normalized)
plt.figure(figsize=(12, 8), dpi=120)

ind_array = np.arange(len(labels))
x, y = np.meshgrid(ind_array, ind_array)

for x_val, y_val in zip(x.flatten(), y.flatten()):
    c = cm_normalized[y_val][x_val]
#     if c > 0.01:
    plt.text(x_val, y_val, "%0.2f" % (c,), color='grey', fontsize=12, va='center', ha='center')
# offset the tick
plt.gca().set_xticks(tick_marks, minor=True)
plt.gca().set_yticks(tick_marks, minor=True)
plt.gca().xaxis.set_ticks_position('none')
plt.gca().yaxis.set_ticks_position('none')
plt.grid(True, which='minor', linestyle='-')
plt.gcf().subplots_adjust(bottom=0.15)

plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')
# show confusion matrix
plt.savefig('/content/svm_confusion_matrix.png', format='png')

"""## Logistics Regression"""

#logistic regresssion
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(random_state=0)
lr.fit(X_train, y_train)
y_pred_lr=lr.predict(X_test)

#adjust parameters
model = LogisticRegression(random_state=0)
param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'penalty': ['l1','l2']}    
grid_search = GridSearchCV(model, param_grid, n_jobs = 8, verbose=1)    
grid_search.fit(X_train, y_train)    
best_parameters = grid_search.best_estimator_.get_params()    
for para, val in list(best_parameters.items()):    
    print(para, val)   
lr = LogisticRegression( C=best_parameters['C'], penalty=best_parameters['penalty'])
#train LR model
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
dump(lr,"lr_try.joblib")

print('Test accuracy',accuracy_score(y_test, y_pred_lr))

cm = confusion_matrix(y_test, y_pred_lr)
np.set_printoptions(precision=2)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
print (cm_normalized)
plt.figure(figsize=(12, 8), dpi=120)

ind_array = np.arange(len(labels))
x, y = np.meshgrid(ind_array, ind_array)

for x_val, y_val in zip(x.flatten(), y.flatten()):
    c = cm_normalized[y_val][x_val]
#     if c > 0.01:
    plt.text(x_val, y_val, "%0.2f" % (c,), color='grey', fontsize=12, va='center', ha='center')
# offset the tick
plt.gca().set_xticks(tick_marks, minor=True)
plt.gca().set_yticks(tick_marks, minor=True)
plt.gca().xaxis.set_ticks_position('none')
plt.gca().yaxis.set_ticks_position('none')
plt.grid(True, which='minor', linestyle='-')
plt.gcf().subplots_adjust(bottom=0.15)

plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')
# show confusion matrix
plt.savefig('/content/lg_confusion_matrix.png', format='png')

"""## Bayes"""

import sklearn.naive_bayes as nb
bayes = nb.GaussianNB()
bayes.fit(X_train, y_train)
pred=bayes.predict(X_test)
print('Test accuracy',accuracy_score(y_test, pred))
# dump(lr,"bayes.joblib")

cm = confusion_matrix(y_test, pred)
np.set_printoptions(precision=2)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
print (cm_normalized)
plt.figure(figsize=(12, 8), dpi=120)

ind_array = np.arange(len(labels))
x, y = np.meshgrid(ind_array, ind_array)

for x_val, y_val in zip(x.flatten(), y.flatten()):
    c = cm_normalized[y_val][x_val]
#     if c > 0.01:
    plt.text(x_val, y_val, "%0.2f" % (c,), color='grey', fontsize=12, va='center', ha='center')
# offset the tick
plt.gca().set_xticks(tick_marks, minor=True)
plt.gca().set_yticks(tick_marks, minor=True)
plt.gca().xaxis.set_ticks_position('none')
plt.gca().yaxis.set_ticks_position('none')
plt.grid(True, which='minor', linestyle='-')
plt.gcf().subplots_adjust(bottom=0.15)

plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')
# show confusion matrix
plt.savefig('/content/bayes_matrix.png', format='png')

"""## CNN"""

# from gensim.models.word2vec import Word2Vec
# model = Word2Vec(sentences, size=128, window=5, min_count=5, workers=4)

X_train = np.array(X_train)
X_test = np.array(X_test)
X_train.shape[0]
X_train.shape[1]
X_train.shape[2]
x_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1],  X_train.shape[2])
x_test = X_test.reshape(X_test.shape[0], 1,  X_train.shape[1],  X_train.shape[2])

mean = np.mean(x_train)
std = np.std(x_train)
x_train -= mean
x_test -= mean
x_train /= std
x_test /= std

import keras.backend.tensorflow_backend as tfback
import tensorflow as tf
def _get_available_gpus():  

    if tfback._LOCAL_DEVICES is None:  
        devices = tf.config.list_logical_devices()  
        tfback._LOCAL_DEVICES = [x.name for x in devices]  
    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]


tfback._get_available_gpus = _get_available_gpus

tf.config.experimental.list_physical_devices(device_type=None)

#CNN
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger
from keras.optimizers import Adam, SGD
# set parameters:
batch_size = 16
n_filter = 10
filter_length = 5
nb_epoch = 100
n_pool = 4
num_feature = 149
vector_size = 300


# build sequential model
model = Sequential()
model.add(Convolution2D(n_filter,filter_length,padding='same',
                        input_shape=(1, num_feature, vector_size),data_format = 'channels_first'))
model.add(Activation('relu'))
model.add(Convolution2D(n_filter,filter_length,filter_length))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(n_pool, n_pool)))
# model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
# model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('softmax'))
# compile model
# model.compile(loss='mse',
#               optimizer='adadelta',
#               metrics=['accuracy'])
# model.compile(optimizer='SDG',
#               loss='binary_crossentropy',
#               metrics=['accuracy'])
# keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=1e-6)	
# optimizer = get_optimizer('SGD')# Choice : SGD, RMSprop, Adadelta, Adam, Adamax, Nadam
optimizer = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)
model.compile(optimizer= optimizer, loss='binary_crossentropy',
                          metrics=['accuracy'])
earlyStopping = EarlyStopping(monitor='accuracy', patience=6, verbose=1,
                                          mode='max')
reduce_lr_loss = ReduceLROnPlateau(monitor='accuracy', factor=0.1,
                                               patience=3, verbose=1, epsilon=1e-4,
                                               mode='max')

model.summary()
model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,
          verbose=1,callbacks=[reduce_lr_loss,earlyStopping])
score = model.evaluate(x_test, y_test, verbose=1)
print('Test score:', score[0])
print('Test accuracy:', score[1])
model.save("cnn.h5")

"""## Combine part"""

from joblib import load
svm_best = load("svm_best.joblib")
lr_best = load("lr_best.joblib")
from sklearn.svm import LinearSVC
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import StackingClassifier
estimators=[('SVC', svm_best), ('LR', lr_best)]

ensemble = VotingClassifier(estimators, voting='soft',weights=[0.5,1])
ensemble.fit(X_train, y_train)
e_pred = ensemble.predict(X_test)
print('Test accuracy',accuracy_score(y_test,e_pred))

ensemble = StackingClassifier(estimators,final_estimator=LinearSVC(random_state=44))
ensemble.fit(X_train, y_train)
e_pred = ensemble.predict(X_test)
print('Test accuracy',accuracy_score(y_test,e_pred))

cm = confusion_matrix(y_test, e_pred)
np.set_printoptions(precision=2)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
print (cm_normalized)
plt.figure(figsize=(12, 8), dpi=120)

ind_array = np.arange(len(labels))
x, y = np.meshgrid(ind_array, ind_array)

for x_val, y_val in zip(x.flatten(), y.flatten()):
    c = cm_normalized[y_val][x_val]
#     if c > 0.01:
    plt.text(x_val, y_val, "%0.2f" % (c,), color='grey', fontsize=12, va='center', ha='center')
# offset the tick
plt.gca().set_xticks(tick_marks, minor=True)
plt.gca().set_yticks(tick_marks, minor=True)
plt.gca().xaxis.set_ticks_position('none')
plt.gca().yaxis.set_ticks_position('none')
plt.grid(True, which='minor', linestyle='-')
plt.gcf().subplots_adjust(bottom=0.15)

plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')
# show confusion matrix
plt.savefig('/content/combine_matrix.png', format='png')

"""# Suggestion Part"""

from joblib import load
from keras.models import load_model

cmodel = load_model('cnn.h5')
cmodel

# Load our word2vec model
import gensim
num_features = 300
w2v_model = gensim.models.word2vec.Word2Vec.load("word2vec_model_trained_on_yelp_review"+str(num_features)+".w2v")
print("Model loaded")

import numpy as np
other_restaurants =[]
rs = []
with open("brazil_reviews.txt","r") as f:
    for line in f.readlines():
        if len(line) >0:
            sum_w2v = np.zeros(num_features,dtype=float)
            r, c = line.split(",")
            for w in c.split(" "):
                if w in w2v_model:
                    sum_w2v += w2v_model[w]
            rs.append(r)
            other_restaurants.append(np.array(sum_w2v))

other_restaurants

lr = load("lr_best.joblib")
lr_recommend=lr.predict(other_restaurants)
lr_recommend

import json
suggest=dict()
for i in range(len(rs)):
    suggest[rs[i]]=str(lr_recommend[i])
with open('recommend_restaurants_r.json', 'w') as outfile:  
    json.dump(suggest, outfile)

suggests=dict()
for i in range(len(rs)):
    if lr_recommend[i]==5:
        suggests[rs[i]]='Good'
    elif lr_recommend[i]==3:
        suggests[rs[i]]='Medium'
    else:
        suggests[rs[i]]='Bad'
with open('recommend_restaurants_l.json', 'w') as outfile:  
    json.dump(suggests, outfile)

import json
import json
import csv
import codecs
f = open('recommend_restaurants_l.json')
data = json.load(f)
f.close()
data

with open('/content/suggest.csv', 'w') as f:  
    for i in data:
        f.write(i+','+data[i]+'\n')